{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3de095fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:58:33.264351Z",
     "start_time": "2025-12-14T17:58:13.616728Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 files matching pattern 'acidentes*.csv'\n",
      "Processing 2009...\n",
      "  -> Saved 2009.csv\n",
      "Processing 2014...\n",
      "  -> Saved 2014.csv\n",
      "Processing 2020...\n",
      "  -> Saved 2020.csv\n",
      "Processing 2012...\n",
      "  -> Saved 2012.csv\n",
      "Processing 2019...\n",
      "  -> Saved 2019.csv\n",
      "Processing 2015...\n",
      "  -> Saved 2015.csv\n",
      "Processing 2024...\n",
      "  -> Saved 2024.csv\n",
      "Processing 2007...\n",
      "  -> Saved 2007.csv\n",
      "Processing 2022...\n",
      "  -> Saved 2022.csv\n",
      "Processing 2023...\n",
      "  -> Saved 2023.csv\n",
      "Processing 2017...\n",
      "  -> Saved 2017.csv\n",
      "Processing 2021...\n",
      "  -> Saved 2021.csv\n",
      "Processing 2013...\n",
      "  -> Saved 2013.csv\n",
      "Processing 2016...\n",
      "  -> Saved 2016.csv\n",
      "Processing 2011...\n",
      "  -> Saved 2011.csv\n",
      "Processing 2008...\n",
      "  -> Saved 2008.csv\n",
      "Processing 2025...\n",
      "  -> Saved 2025.csv\n",
      "Processing 2010...\n",
      "  -> Saved 2010.csv\n",
      "Processing 2018...\n",
      "  -> Saved 2018.csv\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 1. Define the pattern and columns we need\n",
    "file_pattern = \"acidentes*.csv\"\n",
    "columns_to_keep = ['id', 'dia_semana', 'horario', 'mortos']\n",
    "\n",
    "# 2. Find all matching files\n",
    "files = glob.glob(file_pattern)\n",
    "print(f\"Found {len(files)} files matching pattern '{file_pattern}'\")\n",
    "\n",
    "for file_path in files:\n",
    "    # 3. Extract the year from the filename using Regex\n",
    "    # Matches \"acidentes2020_\" -> group 1 is \"2020\"\n",
    "    match = re.search(r'acidentes(\\d{4})', file_path)\n",
    "    \n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        output_filename = f\"{year}.csv\"\n",
    "        \n",
    "        print(f\"Processing {year}...\")\n",
    "        \n",
    "        try:\n",
    "            # 4. Read the specific columns\n",
    "            # We use sep=';' and encoding='latin-1' as per your original files\n",
    "            df = pd.read_csv(file_path, sep=';', encoding='latin-1', usecols=columns_to_keep)\n",
    "            df.sort_values(by='mortos', ascending=False, inplace=True)\n",
    "            df.drop_duplicates(subset=['id'], keep='first', inplace=True)\n",
    "\n",
    "            # 5. Save to a new, smaller CSV\n",
    "            # We keep sep=';' to ensure the HTML parser (which expects semicolons) works perfectly\n",
    "            df.to_csv(output_filename, index=False, sep=';')\n",
    "            \n",
    "            print(f\"  -> Saved {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error processing {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Skipping {file_path} (could not extract year from filename)\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "172c1b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:58:52.626848Z",
     "start_time": "2025-12-14T17:58:45.522376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 year files.\n",
      "Processing 2014...\n",
      " -> Saved heatmap_2014.csv (168 rows)\n",
      "Processing 2008...\n",
      " -> Saved heatmap_2008.csv (168 rows)\n",
      "Processing 2016...\n",
      " -> Saved heatmap_2016.csv (168 rows)\n",
      "Processing 2009...\n",
      " -> Saved heatmap_2009.csv (168 rows)\n",
      "Processing 2021...\n",
      " -> Saved heatmap_2021.csv (168 rows)\n",
      "Processing 2013...\n",
      " -> Saved heatmap_2013.csv (168 rows)\n",
      "Processing 2007...\n",
      " -> Saved heatmap_2007.csv (168 rows)\n",
      "Processing 2015...\n",
      " -> Saved heatmap_2015.csv (168 rows)\n",
      "Processing 2012...\n",
      " -> Saved heatmap_2012.csv (168 rows)\n",
      "Processing 2017...\n",
      " -> Saved heatmap_2017.csv (168 rows)\n",
      "Processing 2024...\n",
      " -> Saved heatmap_2024.csv (168 rows)\n",
      "Processing 2020...\n",
      " -> Saved heatmap_2020.csv (168 rows)\n",
      "Processing 2011...\n",
      " -> Saved heatmap_2011.csv (168 rows)\n",
      "Processing 2010...\n",
      " -> Saved heatmap_2010.csv (168 rows)\n",
      "Processing 2018...\n",
      " -> Saved heatmap_2018.csv (168 rows)\n",
      "Processing 2019...\n",
      " -> Saved heatmap_2019.csv (168 rows)\n",
      "Processing 2022...\n",
      " -> Saved heatmap_2022.csv (168 rows)\n",
      "Processing 2025...\n",
      " -> Saved heatmap_2025.csv (168 rows)\n",
      "Processing 2023...\n",
      " -> Saved heatmap_2023.csv (168 rows)\n",
      "Done! You can now run the HTML visualization.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "\n",
    "# 1. Define Standard Names\n",
    "# We normalize the text to remove accents/special characters first.\n",
    "# This makes matching \"Sábado\", \"sabado\", \"SÁBADO\", and encoding errors like \"SÃ¡bado\" much simpler.\n",
    "def normalize_day(val):\n",
    "    if not isinstance(val, str):\n",
    "        return None\n",
    "    \n",
    "    # Lowercase and strip whitespace\n",
    "    s = val.lower().strip()\n",
    "    \n",
    "    # Normalize unicode characters (decomposes accents, e.g., 'á' becomes 'a' + '´')\n",
    "    # Then encode to ASCII (ignoring non-ascii parts like accents or encoding artifacts)\n",
    "    s_clean = unicodedata.normalize('NFKD', s).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Check for unique substrings in the cleaned text\n",
    "    if 'dom' in s_clean: return 'domingo'\n",
    "    if 'seg' in s_clean: return 'segunda-feira'\n",
    "    if 'ter' in s_clean: return 'terça-feira'\n",
    "    if 'qua' in s_clean: return 'quarta-feira'\n",
    "    if 'qui' in s_clean: return 'quinta-feira'\n",
    "    if 'sex' in s_clean: return 'sexta-feira'\n",
    "    \n",
    "    # \"sab\" will now match 'sabado', 'sábado' (cleaned to 'sabado'), \n",
    "    # and even 'sÃ¡bado' (cleaned effectively to 'sabado')\n",
    "    if 'sab' in s_clean: return 'sábado'\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 2. Find the year files (e.g., 2024.csv)\n",
    "files = glob.glob(\"[0-9][0-9][0-9][0-9].csv\")\n",
    "print(f\"Found {len(files)} year files.\")\n",
    "\n",
    "for file_path in files:\n",
    "    year = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    print(f\"Processing {year}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read the file\n",
    "        df = pd.read_csv(file_path, sep=';', encoding='latin-1')\n",
    "        \n",
    "        # Apply the normalization function\n",
    "        df['dia_semana'] = df['dia_semana'].apply(normalize_day)\n",
    "        \n",
    "        # Drop rows where day mapping failed\n",
    "        df = df.dropna(subset=['dia_semana']) \n",
    "\n",
    "        # Extract Hour (taking first part of HH:MM:SS)\n",
    "        df['hour'] = pd.to_numeric(df['horario'].astype(str).str.split(':').str[0], errors='coerce')\n",
    "        df = df.dropna(subset=['hour'])\n",
    "        df['hour'] = df['hour'].astype(int)\n",
    "\n",
    "        # Clean 'mortos' column (handle commas if present)\n",
    "        if df['mortos'].dtype == object:\n",
    "            df['mortos'] = df['mortos'].astype(str).str.replace(',', '.').astype(float)\n",
    "            \n",
    "        # Group and Sum\n",
    "        heatmap = df.groupby(['dia_semana', 'hour'])['mortos'].sum().reset_index()\n",
    "        \n",
    "        # Save tiny heatmap file\n",
    "        output_filename = f\"heatmap_{year}.csv\"\n",
    "        heatmap.to_csv(output_filename, index=False, sep=';')\n",
    "        print(f\" -> Saved {output_filename} ({len(heatmap)} rows)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "print(\"Done! You can now run the HTML visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cae96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa2e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
